<!DocType html>
<html lang="en">
<head>
    <meta charset="UTF-8>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

      aside {text-align: center;}
      div {text-align: center;}

        body {
          height: 100%;
          padding: 0;
          margin: 0;
          font-family: Arial, Helvetica, sans-serif;
        }

        h1{
          text-align: center
        }

        #imgcenter{
          display: block;
          margin-left: auto;
          margin-right: auto;
        }

        hr.new1{
          text-align:center;
          width: 1210px;
        }
        hr.new2{
          text-align:center;
          width: 1210px;
        }

        tr{
          text-align:center;
        }
        
        .center{
          margin: auto;
        }

        .container{
          text-align: center;
        }

        .topnav {
          overflow: hidden;
          background-color: #333;
        }
        
        .topnav a {
          float: left;
          color: #f2f2f2;
          text-align: center;
          padding: 14px 16px;
          text-decoration: none;
          font-size: 17px;
        }
        
        .topnav a:hover {
          background-color: #ddd;
          color: black;
        }
        
        .topnav a.active {
          background-color: #0c86f894;
          color: white;
        }
        #pBox1 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 75px;
        }
        #pBox2 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 1000px; 
        }
        #pBox3 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 850px;
        }
        #pBox4 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 350px;
        }
        #pBox5 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 790px;
        }
        #pBox6 {
         position:relative; left: 20px;
         border-radius: 8px;
         background: #bdbdbd;
         padding: 10px; 
         width: 1200px;
         height: 450px;
        }
        .source{
          position:relative; left: 560px;
          color: #0c60afc5;
          font-size:small;
        }
    </style>
    
</head>


<body style="background-color:rgb(218, 243, 243);">
    <nav class="topnav">
        <a href="index.html#home">Home</a>
        <a class="active" href="HowNNs.html#how">How ANNs work</a>
        <a href="MakeNNs.html#tutorial">Make your own ANNs</a>
        <a href="Refs.html#refs">Acknowledgements/Further Resources</a>
    </nav>
<!--Uses of NNs hyperlinks-->
<aside>
  <br>
  <a href="#intro">Introduction to how ANNs work</a> &nbsp &nbsp
  <a href="#background">Background to how ANNs work</a> &nbsp &nbsp
  <a href="#perceptron">Perceptron Example</a> &nbsp &nbsp
  <a href="#layering">Multilayering ANNs</a> &nbsp &nbsp
  <a href="#layering">Other Neural Networks Examples</a> &nbsp &nbsp
</aside>

<!--Title and Introduction Section-->
<section>
  <h1 id ="intro">A Look Behind The Curtain</h1>
  <hr class="new1">
  <p id="pBox1">
    "Birds inspired us to fly, burdock plants inspired velcro, and countless more inventions were inspired by nature. It seems only logical, then, to look at the brain's
    architecture for inspiration on how to build an intelligent machine." - Aurelion Geron.
    <br>
    This is the key idea behind the innovation known as ANNs or Artificial Neural Networks.
 

  </p >
</section>
<!--Text Section-->
<section>

  <!--Explanation on how some ANNS work (Likely Brief)-->
  <h1 id="background">Background on how ANNS work</h1>
  <hr class = "new2">
  <p id="pBox2">The first documented case of this innovation being advanced upon was when the forefathers of Neural Networks: Warren McCulloch and Walter Pitts proposed a very simple artificial neuron. 
    This artificial neuron consisted of one or more binary (on/off) inputs and one binary output. The output of the neuron operates when enough of its inputs are triggered. 
    Even with such a simple neuron model it is possible to perform some logical computations:
    <br>
    <br>
    <!--INSERT PIC-->
    <img id = "imgcenter" src="ANNs-performing-simple-logical-computations-2.png" alt="logic"
    width = "600"
    length = "500">
    <br>
    <span class="source">Source: https://www.researchgate.net/figure/ANNs-performing-simple-logical-computations-2_fig2_351547862 </span>
    <br>
    Seeing how this is a neuron from the mid 1950s it’s natural for the current artificial neurons to have improved. 
    Generally artificial neurons add together the values of its inputs and if this value is above a threshold it sends its own signal to its output which is further received by other neurons
    <br>
    The neuron doesn’t need to treat its inputs equally and can assign each a weight for which ones it values more or which value it doesn’t value by assigning it a negative weight. 
    Each neuron is connected with other neurons whose values are weighted, strengthening or weakening the signals going through the neural network.
    <br>
    Typically the training process involves adjusting the weight values so that the final product of the network is correct. 
    Usually this is in the form of using a method known as Gradient Descent. 
    To explain in the a vague but easy to understand way basically a neural network makes small changes to weights and biases to create small improvements and keeps doing this until it can’t improve anymore:
    <br> <br>
    <!--INSERT PIC-->
    <img id = "imgcenter" src="Trainingloop.png" alt="training"
      width = "900"
      length = "800">
    <br>
    <span class="source">Source: https://towardsdatascience.com/the-beginners-glossary-of-neural-network-terms-a9617354078 </span>
    
    <br>
    In short, ANNS are a stack (group/chain) of simple learning algorithms (neurons) that  process the input, and then produce an output. 
    This embodies the idea of deep learning by design, as each layer of neurons learns a more refined understanding of the input.
    <br>
  </p>

  <!--Interactive Perceptron-->
  <h1 id="perceptron">Perceptron Example and How it works</h1>
  <hr class = "new2">
  <p id="pBox3">
    The Perceptron is one of the simplest ANN architectures. It's based on a slightly different artificial neuron known as a "Threshold Logic Unit" (TLU). 
    Instead of binary on/off values for the input and output it is numbers instead. 
    <br> <br>
    Most inputs are associated with a weight value. However sometimes a perceptron/neural network may include an input not associated with a weight value: a bias value. 
    This bias value allows for a neural network to move more than just its weights to maximize accuracy. 
    <br>
    Then the TLU computes a weighted sum of all of its inputs including bias (Sum = w1*x1 + w2*x2 + w3*x3 + ... + wn+xn + b). Then this
    sum is put into either a step function or a signum function. The output is the result of the sum put into the function.
    <br> <br>
    <img id = "imgcenter" src="Perceptron.png" alt="perceptron" width = "600"
    length ="400">
    <br> <br> 
    Typically when you start using a perceptron by default you set the weights and bias to 0 and the accuracy of the predictions
    is not great. To fix this issue the perceptron is trained which basically just involves finding the right weight values using a training algorithm.
    Following the Gradient Descent method, for every prediction incorrect the training algorithm updates the weight values and bias value of the perceptron.
    <br>
    <br>
    How do these values update exactly? 
    The weight updates through the use of the formula: (wi = wj + lr(y-yhat)*x) amd to update the bias: (b = bj + lr(y-yhat)
    <br><br>
       wi represents which weight is being changed <br>
       wj represents the previous weight (may equal 0 on the first pass of the formula) <br>
       lr represents the learning rate <br>
       y represents the target response <br>
       yhat represents the predicted response <br>
       x represents the respective weights' corresponding input value. <br>
       bj representing the previous bias value (may equal 0 on first pass) <br> <br>

      These formulas are used for each weight and bias for as many times as there were errors in the output. 
      Also each weight from a previous use of the formula carries on over to the next use of the formula
      <br> <br>
       Next we'll show a working perceptron example to help illustrate the ideas discussed.
    
  </p>
    <!--PERCEPTRON SECTION-->
    <table class="center">
      <tr><!--ROW1 for week/day and days header-->
        <th>Week/Day</th>
        <th>Monday</th>
        <th>Tuesday</th>
        <th>Wednesday</th>
        <th>Thursday</th>
        <th>Friday</th>
        <th>Saturday</th>
        <th>Sunday</th>
      </tr>
      <tr><!--Week 1 ROW-->
        <th>Week 1:</th>
        <td><div id="w1d1"></div><img src="sunny.png" alt="sunny"><br>75°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td><!--75°, 1 windy, 0mm, sunny-->
        <td><div id="w1d2"></div><img src="sunny.png" alt="sunny"><br>98°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--98°, 3 windy, 0mm, sunny-->
        <td><div id="w1d3"></div><img src="cloudy.png" alt="cloudy"><br>70°<br>0.3mm rain/hr<br><img src="wind.png" alt="wind"></td><!--70°, 1 windy, 0.3mm, cloudy-->
        <td><div id="w1d4"></div><img src="rainy.png" alt="rainy"><br>60°<br>7.0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--60°, 2 windy, 7.0mm, rainy-->
        <td><div id="w1d5"></div><img src="stormy.png" alt="stormy"><br>75°<br>3.0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--75°, 3 windy, 3.0mm, stormy-->
        <td><div id="w1d6"></div><img src="cloudy.png" alt="cloudy"><br>80°<br>0.3mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--80°, 2 windy, 0.3mm, cloudy-->
        <td><div id="w1d7"></div><img src="sunny.png" alt="sunny"><br>80°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td><!--80°, 1 windy, 0mm, sunny-->
      </tr>
      <tr><!--Week 2 ROW-->
        <th>Week 2:</th>
        <td><div id="w2d1"></div><img src="sunny.png" alt="sunny"><br>72°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d2"></div><img src="cloudy.png" alt="cloudy"><br>30°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d3"></div><img src="stormy.png" alt="stormy"><br>59°<br>6.3mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d4"></div><img src="rainy.png" alt="rainy"><br>52°<br>3.2mm rain/hr<br><img src="wind.png" alt="wind"></td>
        <td><div id="w2d5"></div><img src="rainy.png" alt="rainy"><br>55°<br>1.5mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d6"></div><img src="sunny.png" alt="sunny"><br>60°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d7"></div><img src="sunny.png" alt="sunny"><br>69°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td>
      </tr>

    </table>


    <!--Button Section-->
    <div id="robot"></div>
    <form class="container">  
      <input type="button" value="Create Perceptron" onclick="start()"/> 
      <input type="button" value="250 Iterations" onclick="iterate()"/> 
    </form>  

    <!--Load the perceptron code-->
    <script src="perceptron.js"></script>

    <div id='iterations'></div><!--Display the iteration count-->
    <div id="bias"></div><!--Display the bias-->
    <div id='weights'></div><!--Display the weights-->
    <div id='accuracy'></div><!--Display the accuracy-->
    <!-- Insert The perceptron model above -->
    <p id= "pBox4">
      How does this perceptron model work? 
      <br>
        As you can see this perceptron model has four binary inputs (X1,X2,X3,X4)
        <br>
        X1: The Overcast of the Day, Range:[-2 to 1] where -2 is lightning storm, -1 is a rainstorm, 0 is cloudy and 1 is sunny <br>
        X2: The Average Temperature of the Day in Fahreheit <br>
        X3: The Average Windyness of the Day with a Range:[1 to 3] where 1 represents a 0-11 mph, 2 representing 12-22 mph windspeed and 3 representing Windspeed over 22 mph <br>
        X4: The Average Amount of Precipitation expected in millimeters <br> <br>

        The y-hat and y values of either 0 or 1 represents "No it's not nice enough to take a walk outside" 
        and "Yes it's nice enough to take a walk outside" respectively.
        <br> <br>
        Each of its inputs has a weight associated with it and there is also a bias value inplace too. 
        <br> For every iteration it measures the accuracy of the model and uses the gradient descent method to tweak the weights to get a better accuracy
        <br> <br>
        Around 22000 the model is 100% accurate to the training set. <br>
        Typically the training set should be much larger than 14 sets of data so the model can give more accurate predictions.
        <br> <br>
        Even if this neural network is incredibly simple it's still extremely cool as we basically trained a model 
        to emulate the same calculations we do when we wake up and look at the weather app.
      
  </p>

  <!--Multilayering-->
  <h1 id="layering">Multilayer ANNs</h1>
  <hr class = "new2">
  <p id = "pBox5">
    While linear problems are no problem for a single-layer perceptron the performance of the perceptron on more complex non-linear problems was a huge roadblock for the advancement of ANNs. 
    To solve these more complex problems a neural network needs more than just one layer of neurons. The multiple layers added are referred to as “hidden” layers, since it does not constitute the input or output layer. 
    When an ANN has a large amount of hidden layers it’s called a “Deep Neural Network” hence the field of Deep Learning involves these ANNs.
    <br>
    What makes implementing multilayer neural networks more challenging is that these networks require much more than a simple gradient descent algorithm to train its weights. 
    What finally got the development of Neural Networks over this hump was the introduction of a “backpropagation algorithm”. As Aurelion Geron in Hands-On Machine Learning describes it “in short it is simply Gradient Descent using an efficient technique to compute the gradients automatically” in regards to every single model parameter. 
    <br>
    <br>
    This algorithm involves one training set at a time and it sends the full training set multiple times to the first hidden layer. 
    The Algorithm then computes the output of all the neurons in this layer and passes it onto the next layer until it reaches the output layer 
    This is referred to as a forward pass which is just like making predictions but preserving all results at every step to use it in the second step: the backwards pass.
    <br>
    Next the algorithm measures the error of the process using a loss function. 
    Then it computes how much each output connection contributed to this error by applying the chain rule. The chain rule is from calculus and refers to finding the derivative of a composite function. 
    <br>
    The next step involves measuring how much error contribution came from each connection at a lower layer using the chain rule until the algorithm reaches the input layer. This backwards pass method efficiently measures the error gradient across all connection weights by using error gradient backward through the network.
    Finally the algorithm performs a gradient descent step to change all the weights in the network using the error gradients. 
    <br>
    <img id = "imgcenter" src="backpropagation.png" alt="backprog"
    width = "500"
    length = "300">
    <br>
    Credit to Aurelion Geron’s explanation of backpropagation in a relatively simple way.
    If you want to see a much better explanation of backpropagation but in a much more complex way here is an article for you.
    <br>
    <!--INSERT LINK-->
    https://towardsdatascience.com/understanding-backpropagation-abcc509ca9d0
    <br>
    <br>
    Now with a way to properly train weights implementing a multilayer neural network is possible and viable.
  </p>

  <!--Last Addition-->
  <h1 id="other">Other Neural Networks</h1>
  <hr class = "new2">
  <p id="pBox6">
    Before you're done learning about the basics of Neural Networks we felt it was valueable to at least introduce some other neural networks to you
    <br>
    Everything from this page mostly refers to a “feed-forward” type neural network, there are multiple different types and classes of neural networks that we unfortunately do not have the time to touch upon. 
    However we feel there are two other classes of Neural Networks that are worth touching upon, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).
    Here's a table to give a very very basic idea of some of the abilities and differences between the neural networks.
    <!--INSERT PIC-->
    <br>
    <img id = "imgcenter" src="OtherNNs.png" alt="OtherNN">
  </p>

</section>

</body>

</html>
