<!DocType html>
<html lang="en">
<head>
    <meta charset="UTF-8>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body {
          margin: 0;
          font-family: Arial, Helvetica, sans-serif;
        }
        
        .topnav {
          overflow: hidden;
          background-color: #333;
        }
        
        .topnav a {
          float: left;
          color: #f2f2f2;
          text-align: center;
          padding: 14px 16px;
          text-decoration: none;
          font-size: 17px;
        }
        
        .topnav a:hover {
          background-color: #ddd;
          color: black;
        }
        
        .topnav a.active {
          background-color: #04AA6D;
          color: white;
        }
    </style>
    
</head>


<body>
    <nav class="topnav">
        <a href="index.html#home">Home</a>
        <a class="active" href="HowNNs.html#how">How ANNs work</a>
        <a href="MakeNNs.html#tutorial">Make your own ANNs</a>
        <a href="Refs.html#refs">Acknowledgements/Further Resources</a>
    </nav>
<!--Uses of NNs hyperlinks-->
<aside style="float:right;">
  <br>
  <a href="#intro">Introduction to how ANNs work</a> <br>
  <a href="#background">Background to how ANNs work</a> <br>
  <a href="#perceptron">Perceptron Example</a> <br>
  <a href="#layering">Multilayering ANNs</a> <br>
  <a href="#layering">Other Neural Networks Examples</a> <br>
</aside>

<!--Title and Introduction Section-->
<section>
  <h1 id ="intro">A Look Behind The Curtain</h1>
  <p>
    "Birds inspired us to fly, burdock plants inspired velcro, and countless more inventions were inspired by nature. It seems only logical, then, to look at the brain's
    architecture for inspiration on how to build an intelligent machine." - Aurelion Geron.
    <br>
    This is the key idea behind the innovation known as ANNs or Artificial Neural Networks.
 

  </p>
</section>
<!--Text Section-->
<section>

  <!--Explanation on how some ANNS work (Likely Brief)-->
  <h1 id="background">Background on how ANNS work</h1>
  <p>The first documented case of this innovation being advanced upon was when the forefathers of Neural Networks: Warren McCulloch and Walter Pitts proposed a very simple artificial neuron. 
    This artificial neuron consisted of one or more binary (on/off) inputs and one binary output. The output of the neuron operates when enough of its inputs are triggered. 
    Even with such a simple neuron model it is possible to perform some logical computations:
    <br>
    <!--INSERT PIC-->
    <img src="ANNs-performing-simple-logical-computations-2.png" alt="logic">
    <br>
    <small>Source: https://www.researchgate.net/figure/ANNs-performing-simple-logical-computations-2_fig2_351547862 </small>
    <br>
    Seeing how this is a neuron from the mid 1950s it’s natural for the current artificial neurons to have improved. 
    Generally artificial neurons add together the values of its inputs and if this value is above a threshold it sends its own signal to its output which is further received by other neurons
    <br>
    The neuron doesn’t need to treat its inputs equally and can assign each a weight for which ones it values more or which value it doesn’t value by assigning it a negative weight. 
    Each neuron is connected with other neurons whose values are weighted, strengthening or weakening the signals going through the neural network.
    <br>
    Typically the training process involves adjusting the weight values so that the final product of the network is correct. 
    Usually this is in the form of using a method known as Gradient Descent. 
    To explain in the a vague but easy to understand way basically a neural network makes small changes to weights and biases to create small improvements and keeps doing this until it can’t improve anymore:
    <br>
    <!--INSERT PIC-->
    <img src="Trainingloop.png" alt="training">
    <br>
    <small>Source: https://towardsdatascience.com/the-beginners-glossary-of-neural-network-terms-a9617354078 </small>
    <br>
    In short, ANNS are a stack (group/chain) of simple learning algorithms (neurons) that  process the input, and then produce an output. 
    This embodies the idea of deep learning by design, as each layer of neurons learns a more refined understanding of the input.
    <br>
  </p>

  <!--Interactive Perceptron-->
  <h1 id="perceptron">Perceptron Example and How it works</h1>
  <p>
    NOTES ABOUT BIAS
    ALLOWS THE MODEL TO MOVE AROUND MORE THAN JUST THE WEIGHTS 
    The Perceptron is one of the simplest ANN architectures. It's based on a slightly different
    artificial neuron known as a "Threshold Logic Unit" (TLU). Instead of binary on/off values
    for the input and output it is numbers instead. Each input is also associated with a weight. Then the TLU
    computes a weighted sum of its inputs (Sum = w1*x1 + w2*x2 + w3*x3 + ... + wn+xn). The this
    sum is put into either a step function or a signum function. The output is the result of the sum put into the function.
    PICTURE OF WHOLE PROCESS HERE

    Typically when you start using a perceptron by default you set the weights to 0 and the accuracy of the predictions
    is not great. To fix this issue the perceptron is trained which basically just involves finding the right weight values using a training algorithm.
    For every prediction incorrect the training algorithm updates the weight values of perceptron.
    How does the weight update? 
    It updates through the use of a formula (wi = wi,j + lr(yi-yhati)*xi).
    To give more context before explaining the meaning of the values, this formula is used for each weight for as many times as there were wrong predictions. 
    Also each weight from a previous use of the formula carries on over to the next use of the formula
    #TALK ABOUT BIAS

    wi represents which weight is being changed
    wi,j represents the previous weight
    lr represents the learning rate
    yi represents the target response
    yhat represents the predicted response
    xi represents the respective weights' corresponding input value.

    A Perceptron does not always have to be binary classifications, it can also achieve multi-class classification by
    adding more output layers to match the multiple classes possible.
    PICTURE OF MULTICLASSIFICATION PERCEPTRON HERE

    Next we'll show a binary classifying perceptron example to help illustrate the ideas discussed.

    A Perceptron does not always have to be binary classifications, it can also achieve multi-class classification by
    adding more output layers to match the multiple classes possible.
    PICTURE OF MULTICLASSIFICATION PERCEPTRON HERE

    Next we'll show a binary classifying perceptron example to help illustrate the ideas discussed.

    <!--PERCEPTRON SECTION-->
    <table>
      <tr><!--ROW1 for week/day and days header-->
        <th>Week/Day</th>
        <th>Monday</th>
        <th>Tuesday</th>
        <th>Wednesday</th>
        <th>Thursday</th>
        <th>Friday</th>
        <th>Saturday</th>
        <th>Sunday</th>
      </tr>
      <tr><!--Week 1 ROW-->
        <th>Week 1:</th>
        <td><div id="w1d1"></div><img src="sunny.png" alt="sunny"><br>75°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td><!--75°, 1 windy, 0mm, sunny-->
        <td><div id="w1d2"></div><img src="sunny.png" alt="sunny"><br>98°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--98°, 3 windy, 0mm, sunny-->
        <td><div id="w1d3"></div><img src="cloudy.png" alt="cloudy"><br>70°<br>0.3mm rain/hr<br><img src="wind.png" alt="wind"></td><!--70°, 1 windy, 0.3mm, cloudy-->
        <td><div id="w1d4"></div><img src="rainy.png" alt="rainy"><br>60°<br>7.0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--60°, 2 windy, 7.0mm, rainy-->
        <td><div id="w1d5"></div><img src="stormy.png" alt="stormy"><br>75°<br>3.0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--75°, 3 windy, 3.0mm, stormy-->
        <td><div id="w1d6"></div><img src="cloudy.png" alt="cloudy"><br>80°<br>0.3mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td><!--80°, 2 windy, 0.3mm, cloudy-->
        <td><div id="w1d7"></div><img src="sunny.png" alt="sunny"><br>80°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td><!--80°, 1 windy, 0mm, sunny-->
      </tr>
      <tr><!--Week 2 ROW-->
        <th>Week 2:</th>
        <td><div id="w2d1"></div><img src="sunny.png" alt="sunny"><br>72°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d2"></div><img src="cloudy.png" alt="cloudy"><br>30°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d3"></div><img src="stormy.png" alt="stormy"><br>59°<br>6.3mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d4"></div><img src="rainy.png" alt="rainy"><br>52°<br>3.2mm rain/hr<br><img src="wind.png" alt="wind"></td>
        <td><div id="w2d5"></div><img src="rainy.png" alt="rainy"><br>55°<br>1.5mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d6"></div><img src="sunny.png" alt="sunny"><br>60°<br>0mm rain/hr<br><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"><img src="wind.png" alt="wind"></td>
        <td><div id="w2d7"></div><img src="sunny.png" alt="sunny"><br>69°<br>0mm rain/hr<br><img src="wind.png" alt="wind"></td>
      </tr>

    </table>


    <!--Button Section-->
    <div id="robot"></div>
    <form>  
      <input type="button" value="Create Perceptron" onclick="start()"/> 
      <input type="button" value="250 Iterations" onclick="iterate()"/> 
    </form>  

    <!--Load the perceptron code-->
    <script src="perceptron.js"></script>

    <div id='iterations'></div><!--Display the iteration count-->
    <div id="bias"></div><!--Display the bias-->
    <div id='weights'></div><!--Display the weights-->
    <div id='accuracy'></div><!--Display the accuracy-->
    <!-- Insert The perceptron model above -->


    <br>How does this perceptron model work? 
        As you can see this perceptron model has four binary inputs (X1,X2,X3,X4)
        
        X1 representing the Overcast of the day has a range of -2 - 1 where -2 is lightning storm, -1 is a rainstorm, 0 is cloudy and 1 is sunny
        X2 is pretty self-explanatory representing Temperature in Fahreheit
        X3 is the Windyness of the day with a range of 1 - 3 with 1 representing 0-10 mph, 2 representing 11-20 mph windspeed and 3 representing Windspeed over 21 mph
        X4 represents the amount of precipitation expected in millimeters. 

        with 0 representing no and 1 representing yes it is nice enough to take a walk.


      <! Write about how we just made a computer basically do the same calculations we do when we wake up and look at the weather app and decided whether it's going to be a nice day or not
    <br>paragraph
    <br>paragraph
    <br>paragraph
    <br> While the perceptron is very simple and decent at classification it still has many problems solving some simple problems.
         Take the Exclusive OR classification problem for example. 
         Picture of the problem here. 
         A Single layer Perceptron can not solve this problem.
         However some of the limitations of a Single-layer Perceptron are fixed by stacking multiple perceptrons resulting in 
         Multi-layer (MLP). A MLP for example can solve the Exclusive OR Problem by.
    <br>paragraph
    <br>paragraph
    <br> Moving onto multi-layering
  </p>

  <!--Multilayering-->
  <h1 id="layering">Multilayer ANNs</h1>
  <p>
    While linear problems are no problem for a single-layer perceptron the performance of the perceptron on more complex non-linear problems was a huge roadblock for the advancement of ANNs. 
    To solve these more complex problems a neural network needs more than just one layer of neurons. The multiple layers added are referred to as “hidden” layers, since it does not constitute the input or output layer. 
    When an ANN has a large amount of hidden layers it’s called a “Deep Neural Network” hence the field of Deep Learning involves these ANNs.
    <br>
    What makes implementing multilayer neural networks more challenging is that these networks require much more than a simple gradient descent algorithm to train its weights. 
    What finally got the development of Neural Networks over this hump was the introduction of a “backpropagation algorithm”. As Aurelion Geron in Hands-On Machine Learning describes it “in short it is simply Gradient Descent using an efficient technique to compute the gradients automatically” in regards to every single model parameter. 
    <br>
    This algorithm involves one training set at a time and it sends the full training set multiple times to the first hidden layer. 
    The Algorithm then computes the output of all the neurons in this layer and passes it onto the next layer until it reaches the output layer 
    This is referred to as a forward pass which is just like making predictions but preserving all results at every step to use it in the second step: the backwards pass.
    <br>
    Next the algorithm measures the error of the process using a loss function. 
    Then it computes how much each output connection contributed to this error by applying the chain rule. The chain rule is from calculus and refers to finding the derivative of a composite function. 
    <br>
    The next step involves measuring how much error contribution came from each connection at a lower layer using the chain rule until the algorithm reaches the input layer. This backwards pass method efficiently measures the error gradient across all connection weights by using error gradient backward through the network.
    Finally the algorithm performs a gradient descent step to change all the weights in the network using the error gradients. 
    <br>
    <img src="backpropagation.png" alt="backprog">
    <br>
    CREDIT to Aurelion Geron’s explanation of backpropagation in a relatively simple way.
    If you want to see a much better explanation of backpropagation but in a much more complex way here is an article for you.
    <!--INSERT LINK-->
    <br>
    Now with a way to properly train weights implementing a multilayer neural network is possible and viable.
  </p>

  <!--Last Addition-->
  <h1 id="other">Other Neural Networks</h1>
  <p>
    Before you're done learning about the basics of Neural Networks we felt it was valueable to at least introduce some other neural networks to you
    <br>
    Everything from this page mostly refers to a “feed-forward” type neural network, there are multiple different types and classes of neural networks that we unfortunately do not have the time to touch upon. 
    However we feel there are two other classes of Neural Networks that are worth touching upon, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).
    Here's a table to give a very very basic idea of some of the abilities and differences between the neural networks.
    <!--INSERT PIC-->
    <br>
    <img src="OtherNNs.png" alt="OtherNN">
  </p>

</section>

</body>

</html>
